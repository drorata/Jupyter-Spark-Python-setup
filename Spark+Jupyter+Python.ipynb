{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Spark together with Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will outline how I managed to set up Spark/PySpark in Jupyter/IPython (using Python 3.x). I used as some reference [this post](https://districtdatalabs.silvrback.com/getting-started-with-spark-in-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System initial setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my OS X I installed Python using [Anaconda](https://www.continuum.io/downloads). The default version of Python I have currently installed is 3.4.4 (Anaconda 2.4.0). Note, that I also have installed also 2.x version of Python using `conda create -n python2 python=2.7 anaconda` (see [SO answer](http://stackoverflow.com/a/24415581/671013))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually the simplest step; download the latest binaries from [here](http://spark.apache.org/downloads.html) into `~/Applications` or some other directory of your choice. Next, untar the archive `tar -xzf spark-X.Y.Z-bin-hadoopX.Y.tgz`.\n",
    "\n",
    "For easy access to Spark create a symbolic link to the Spark: \n",
    "\n",
    "```bash\n",
    "ln -s ~/Applications/spark-X.Y.Z-bin-hadoopX.Y ~/Applications/spark\n",
    "```\n",
    "\n",
    "Lastly, add the Spark symbolic link to the `PATH`:\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=~/Applications/spark\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "You can now run Spark/PySpark locally: simply invoke `spark-shell` or `pyspark`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbosity of Spark's output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just execute this command in the spark directory:\n",
    "\n",
    "```bash\n",
    "cp conf/log4j.properties.template conf/log4j.properties\n",
    "```\n",
    "\n",
    "Edit `log4j.properties`:\n",
    "\n",
    "```\n",
    "# Set everything to be logged to the console\n",
    "log4j.rootCategory=WARN, console\n",
    "log4j.appender.console=org.apache.log4j.ConsoleAppender\n",
    "log4j.appender.console.target=System.err\n",
    "log4j.appender.console.layout=org.apache.log4j.PatternLayout\n",
    "log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n\n",
    "\n",
    "# Settings to quiet third party logs that are too verbose\n",
    "log4j.logger.org.eclipse.jetty=WARN\n",
    "log4j.logger.org.eclipse.jetty.util.component.AbstractLifeCycle=ERROR\n",
    "log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=WARN\n",
    "log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=WARN\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use Spark from within a Jupyter notebook, prepand the following to `PYTHONPATH`:\n",
    "\n",
    "```bash\n",
    "export PYTHONPATH=$SPARKHOME/python/lib/py4j-0.8.2.1-src.zip:$SPARKHOME/python/:$PYTHONPATH\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Spark in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a new Jupyter notebook instance:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "inside some directory, say `~/scratch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'pyspark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primes count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isprime(n):\n",
    "    \"\"\"\n",
    "    check if integer n is a prime\n",
    "    \"\"\"\n",
    "    # make sure n is a positive integer\n",
    "    n = abs(int(n))\n",
    "    # 0 and 1 are not primes\n",
    "    if n < 2:\n",
    "        return False\n",
    "    # 2 is the only even prime number\n",
    "    if n == 2:\n",
    "        return True\n",
    "    # all other even numbers are not primes\n",
    "    if not n & 1:\n",
    "        return False\n",
    "    # range starts with 3 and only needs to go up the square root of n\n",
    "    # for all odd numbers\n",
    "    for x in range(3, int(n**0.5)+1, 2):\n",
    "        if n % x == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an RDD of numbers from 0 to 1,000,000\n",
    "nums = sc.parallelize(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the number of primes in the RDD\n",
    "print(nums.filter(isprime).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from: http://langs.eserver.org/the-awful-german-language.txt\n",
    "lines = sc.textFile('./the-awful-german-language.txt',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = lines.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Top 10. http://stackoverflow.com/a/30779026/671013\n",
    "counts.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databricks CSV (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%AddDeps com.databricks spark-csv_2.10 1.2.0 --transitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('file.csv')\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
